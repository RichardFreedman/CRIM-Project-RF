{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "CRIM_Classify_Corpus.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RichardFreedman/CRIM-Project-RF/blob/master/CRIM_Classify_Corpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWRoE9Pj5TQ-"
      },
      "source": [
        "## Classify a Group of Pieces, each to its Own Ouput File\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LjY_UZ-icy_"
      },
      "source": [
        "## Load CRIM Intervals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYMiYPBk5TRE",
        "outputId": "90415593-d0bc-4fc4-d562-a0ac1e35ea94"
      },
      "source": [
        "\n",
        "!pip install crim-intervals\n",
        "from crim_intervals import *\n",
        "import pandas as pd\n",
        "import ast\n",
        "import matplotlib\n",
        "from itertools import tee, combinations\n",
        "import numpy as np\n",
        "from fractions import Fraction\n",
        "import re\n",
        "from tqdm import tqdm\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting crim-intervals\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/26/c38d77b47d8905d4bcd495ce84151b86d6b9b7fc7dc58ff708d0b4487d96/crim_intervals-0.3.7-py3-none-any.whl\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\r\u001b[K     |█████▍                          | 10kB 14.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 20kB 18.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30kB 22.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 40kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 51kB 28.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hCollecting music21<6.0.0,>=5.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/87/19a01af679090c880ed0036f0c221dd7bc3a97f4bddf74e24bc22d1e66cf/music21-5.7.2.tar.gz (18.5MB)\n",
            "\u001b[K     |████████████████████████████████| 18.5MB 229kB/s \n",
            "\u001b[?25hCollecting httpx<0.17.0,>=0.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/c6/59aa4188e7eddb9e89ec67a51598ca6bfc09f1b38c9b45f7ee45af7a4df4/httpx-0.16.1-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->crim-intervals) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->crim-intervals) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->crim-intervals) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->crim-intervals) (2020.12.5)\n",
            "Collecting httpcore==0.12.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ce/544244e2145075896d38f7db3e822c24d1be9f7966ed3d83f158e388d195/httpcore-0.12.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.9MB/s \n",
            "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Collecting h11==0.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/0f/7a0eeea938eaf61074f29fed9717f2010e8d0e0905d36b38d3275a1e4622/h11-0.12.0-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: music21\n",
            "  Building wheel for music21 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for music21: filename=music21-5.7.2-cp37-none-any.whl size=22024603 sha256=201af1716111f4bed4c03c08d46592fccb33a6555a0cc963de20ff8082c8669a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/d0/05/1ef3daa9ae295073d807e468fcd820641965086424f1c633e3\n",
            "Successfully built music21\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: requests, music21, sniffio, h11, httpcore, rfc3986, httpx, crim-intervals\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: music21 5.5.0\n",
            "    Uninstalling music21-5.5.0:\n",
            "      Successfully uninstalled music21-5.5.0\n",
            "Successfully installed crim-intervals-0.3.7 h11-0.12.0 httpcore-0.12.3 httpx-0.16.1 music21-5.7.2 requests-2.25.1 rfc3986-1.4.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paOYBMjSig9r"
      },
      "source": [
        "# Set Basic Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hNIaW5c5TRF"
      },
      "source": [
        "\n",
        "\n",
        "min_exact_matches = 2\n",
        "min_close_matches = 3\n",
        "close_distance = 1\n",
        "vector_size = 4\n",
        "increment_size = 2\n",
        "forward_gap_limit = 35\n",
        "backward_gap_limit = 35\n",
        "min_sum_durations = 10\n",
        "max_sum_durations = 30\n",
        "offset_difference_limit = 500\n",
        "\n",
        "duration_type = \"real\"\n",
        "interval_type = \"generic\"\n",
        "match_type = \"close\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goEd7cXQikk8"
      },
      "source": [
        "## The Following Defines Pathways and Basic Parameters\n",
        "\n",
        "* Must select \"crim\" or \"git\" in the for loop below to depending on where the piece is stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg7DxxtQ5TRG"
      },
      "source": [
        "crim = 'https://raw.githubusercontent.com/CRIM-Project/CRIM-online/master/crim/static/mei/MEI_3.0/'\n",
        "\n",
        "git = 'https://raw.githubusercontent.com/RichardFreedman/CRIM_additional_works/main/'\n",
        "\n",
        "def batch_classify(corpus_titles, duration_type=\"real\", interval_type=\"generic\", match_type=\"close\"):\n",
        "\n",
        "    for title in titles:\n",
        "        path = f\"{git}{title}\"\n",
        "        clean_title = re.search(\"[a-zA-Z_\\d]+\", title).group()\n",
        "        \n",
        "        corpus = CorpusBase([path])\n",
        "#         corpus = CorpusBase(corpus_titles)\n",
        "\n",
        "        if duration_type == \"real\":\n",
        "\n",
        "            vectors = IntervalBase(corpus.note_list)\n",
        "\n",
        "        elif duration_type == \"incremental\":\n",
        "\n",
        "            vectors = IntervalBase(corpus.note_list_incremental_offset(increment_size))\n",
        "\n",
        "        if interval_type == \"generic\":\n",
        "\n",
        "            patterns = into_patterns([vectors.generic_intervals], vector_size)\n",
        "\n",
        "        elif interval_type == \"semitone\":\n",
        "\n",
        "            patterns = into_patterns([vectors.semitone_intervals], vector_size)\n",
        "\n",
        "        if match_type == \"exact\":\n",
        "\n",
        "            exact_matches = find_exact_matches(patterns, min_exact_matches)\n",
        "            output_exact = export_pandas(exact_matches)\n",
        "            df = output_exact\n",
        "            pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "            df[\"note_durations\"] = df[\"note_durations\"].map(lambda x: pd.eval(x))\n",
        "            df[\"start_offset\"] = df[\"start_offset\"].map(lambda x: pd.eval(x))\n",
        "            df[\"end_offset\"] = df[\"end_offset\"].map(lambda x: pd.eval(x))\n",
        "            df[\"pattern_generating_match\"] = df[\"pattern_generating_match\"].apply(tuple)\n",
        "            df[\"pattern_matched\"] = df[\"pattern_matched\"].apply(tuple)\n",
        "            df[\"sum_durs\"] = df.note_durations.apply(sum)\n",
        "            df = df.round(2)\n",
        "\n",
        "        elif match_type == \"close\":\n",
        "\n",
        "            close_matches = find_close_matches(patterns, min_close_matches, close_distance)\n",
        "            output_close = export_pandas(close_matches)\n",
        "            output_close[\"pattern_generating_match\"] = output_close[\"pattern_generating_match\"].apply(tuple)\n",
        "            df = output_close\n",
        "            pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "            df[\"note_durations\"] = df[\"note_durations\"].map(lambda x: pd.eval(x))\n",
        "            df[\"start_offset\"] = df[\"start_offset\"].map(lambda x: pd.eval(x))\n",
        "            df[\"end_offset\"] = df[\"end_offset\"].map(lambda x: pd.eval(x))\n",
        "            df[\"pattern_generating_match\"] = df[\"pattern_generating_match\"].apply(tuple)\n",
        "            df[\"pattern_matched\"] = df[\"pattern_matched\"].apply(tuple)\n",
        "            df[\"sum_durs\"] = df.note_durations.apply(sum)\n",
        "            df = df.round(2)\n",
        "\n",
        "        df2 = df\n",
        "\n",
        "        # Make Groups, Sort By Group and Offset, then and Add Previous/Next\n",
        "        df2[\"group_number\"] = df2.groupby('pattern_matched').ngroup()\n",
        "        df2 = df2.sort_values(['group_number', 'start_offset'])\n",
        "        df2[\"prev_entry_off\"] = df2[\"start_offset\"].shift(1)\n",
        "        df2[\"next_entry_off\"] = df2[\"start_offset\"].shift(-1)\n",
        "\n",
        "\n",
        "        first_of_group = df2.drop_duplicates(subset=[\"pattern_matched\"], keep='first').index\n",
        "        df2[\"is_first\"] = df2.index.isin(first_of_group)\n",
        "        last_of_group = df2.drop_duplicates(subset=[\"pattern_matched\"], keep='last').index\n",
        "        df2[\"is_last\"] = df2.index.isin(last_of_group)\n",
        "\n",
        "        # Check Differences between Next and Last Offset\n",
        "\n",
        "        df2[\"last_off_diff\"] = df2[\"start_offset\"] - df2[\"prev_entry_off\"]\n",
        "        df2[\"next_off_diff\"] = df2[\"next_entry_off\"] - df2[\"start_offset\"]\n",
        "\n",
        "        # Find Parallel Entries \n",
        "        df2[\"parallel\"] = df2[\"last_off_diff\"] == 0\n",
        "\n",
        "        # Set Gap Limits and Check Gaps Forward and Back\n",
        "        df2[\"forward_gapped\"] = df2[\"next_off_diff\"] >= forward_gap_limit\n",
        "        df2[\"back_gapped\"] = df2[\"last_off_diff\"] >= backward_gap_limit\n",
        "\n",
        "        # Find Singletons and Split Groups with Gaps\n",
        "        df2[\"singleton\"] = ((df2['forward_gapped'] == True) & (df2['back_gapped'] == True) | (df2['back_gapped'] == True) & (df2[\"is_last\"]))\n",
        "        df2[\"split_group\"] = (df2['forward_gapped'] == False) & (df2['back_gapped'] == True)\n",
        "\n",
        "        #Mask Out Parallels and Singletons\n",
        "        df2 = df2[df2[\"parallel\"] != True]\n",
        "        df2 = df2[df2[\"singleton\"] != True]\n",
        "        df2[\"next_off_diff\"] = df2[\"next_off_diff\"].abs()\n",
        "        df2[\"last_off_diff\"] = df2[\"last_off_diff\"].abs()\n",
        "\n",
        "        # Find Final Groups\n",
        "        df2[\"combined_group\"] = (df2.split_group | df2.is_first)\n",
        "        df2.loc[(df2[\"combined_group\"]), \"sub_group_id\"] = range(df2.combined_group.sum())\n",
        "        df2[\"sub_group_id\"] = df2[\"sub_group_id\"].ffill()\n",
        "\n",
        "        ###\n",
        "        ### FILTER SHORT OR LONG ENTRIES\n",
        "        ###\n",
        "        df2 = df2[df2[\"sum_durs\"] >= min_sum_durations]\n",
        "        df2 = df2[df2[\"sum_durs\"] <= max_sum_durations]\n",
        "\n",
        "        classified2 = df2.applymap(lists_to_tuples).groupby(\"sub_group_id\").apply(predict_type)\n",
        "\n",
        "        # OPTIONAL:  drop the new singletons\n",
        "\n",
        "        classified2.drop(classified2[classified2['predicted_type'] == \"Singleton\"].index, inplace = True)\n",
        "\n",
        "        # OPTIONAL:  select only certain presentation types\n",
        "\n",
        "        # classified2 = classified2[classified2[\"predicted_type\"] == \"PEN\"]\n",
        "\n",
        "        classified2[\"start\"] = classified2[\"start_measure\"].astype(str) +\"/\"+ classified2[\"start_beat\"].astype(str) \n",
        "        classified2.drop(columns=['start_measure', 'start_beat','offset_diffs'], inplace=True)\n",
        "\n",
        "        # put things back in order by offset and group them again\n",
        "        classified2.sort_values(by = [\"start_offset\"], inplace=True)\n",
        "\n",
        "        # Now transform as Pivot Table\n",
        "        pivot = classified2.pivot_table(index=[\"piece_title\", \"pattern_generating_match\", \"pattern_matched\", \"predicted_type\", \"sub_group_id\"],\n",
        "                    columns=\"entry_number\",\n",
        "                    values=[\"part\", \"start_offset\", \"start\", \"sum_durs\"],\n",
        "                    aggfunc=lambda x: x)\n",
        "        pivot_sort = pivot.sort_values(by = [(\"start_offset\", 1)])\n",
        "        pivot_sort = pivot_sort.fillna(\"-\")\n",
        "        pivot_sort.reset_index(inplace=True)\n",
        "        pivot_sort = pivot_sort.drop(columns=['start_offset', \"sub_group_id\"], level=0)\n",
        "\n",
        "        # group by patterns and minimum of two pieces\n",
        "\n",
        "    #     pivot_sort[\"pattern_matched\"] = pivot_sort.pattern_matched.apply(pd.eval).apply(tuple)\n",
        "\n",
        "    #     pivot_sort[\"unique_titles_for_pattern\"] = pivot_sort.groupby(\"pattern_matched\").piece_title.transform(lambda group: group.nunique())\n",
        "\n",
        "    #     p2 = pivot_sort[pivot_sort.unique_titles_for_pattern > 1]\n",
        "\n",
        "    #     p3 = p2.sort_values(\"pattern_matched\")\n",
        "    #     \n",
        "    #     p3.to_csv(\"corpus_classified.csv\")\n",
        "    #   \n",
        "        pivot_sort.to_csv(f\"{clean_title}_{interval_type}_{match_type}_{duration_type}.csv\")\n",
        "        # pivot_sort.head()\n",
        "        # return pivot_sort\n",
        "   \n",
        "\n",
        "# Converts lists to tuples\n",
        "\n",
        "def lists_to_tuples(el):\n",
        "    if isinstance(el, list):\n",
        "        return tuple(el)\n",
        "    else:\n",
        "        return el\n",
        "\n",
        "# Filters for the length of the Presentation Type in the Classifier\n",
        "\n",
        "def limit_offset_size(array, limit):\n",
        "    under_limit = np.cumsum(array) <= limit\n",
        "    return array[: sum(under_limit)]\n",
        "\n",
        "# Gets the the list of offset differences for each group \n",
        "\n",
        "def get_offset_difference_list(group):\n",
        "    # if we do sort values as part of the func call, then we don't need this first line\n",
        "    group = group.sort_values(\"start_offset\")\n",
        "    group[\"next_offset\"] = group.start_offset.shift(-1)\n",
        "    offset_difference_list = (group.next_offset - group.start_offset).dropna().tolist()\n",
        "    return offset_difference_list\n",
        "\n",
        "# The classifications are done here\n",
        "# be sure to have the offset difference limit set here and matched in gap check below  80 = ten bars\n",
        "\n",
        "def classify_offsets(offset_difference_list):\n",
        "    \"\"\"\n",
        "    Put logic for classifying an offset list here\n",
        "    \"\"\"\n",
        "    # \n",
        "    offset_difference_list = limit_offset_size(offset_difference_list, offset_difference_limit)\n",
        "    \n",
        "    alt_list = offset_difference_list[::2]\n",
        "    \n",
        "    if len(set(offset_difference_list)) == 1 and len(offset_difference_list) > 1:\n",
        "        return (\"PEN\", offset_difference_list)\n",
        "    # elif (len(offset_difference_list) %2 != 0) and (len(set(alt_list)) == 1):\n",
        "    elif (len(offset_difference_list) %2 != 0) and (len(set(alt_list)) == 1) and (len(offset_difference_list) >= 3):\n",
        "        return (\"ID\", offset_difference_list)\n",
        "    elif len(offset_difference_list) >= 1:\n",
        "        return (\"Fuga\", offset_difference_list)\n",
        "    else: \n",
        "        return (\"Singleton\", offset_difference_list)\n",
        "    \n",
        "# adds predicted type, offsets and entry numbers to the results\n",
        "\n",
        "def predict_type(group):\n",
        "    offset_differences = get_offset_difference_list(group)\n",
        "    predicted_type, offsets = classify_offsets(offset_differences)\n",
        "\n",
        "    group[\"predicted_type\"] = [predicted_type for i in range(len(group))]\n",
        "    group[\"offset_diffs\"] = [offsets for i in range(len(group))]\n",
        "    group[\"entry_number\"] = [i + 1 for i in range(len(group))]\n",
        "\n",
        "    return group\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YbaDgbViwP5"
      },
      "source": [
        "\n",
        "## Load MEI Files from CRIM or Github by pasting one or more of [these links](https://docs.google.com/spreadsheets/d/1TzRqnzgcYYuQqZR78c5nizIsBWp4pnblm2wbU03uuSQ/edit?auth_email=rfreedma@haverford.edu#gid=0) below.\n",
        "\n",
        "*Note:  each file must be in quotation marks and separated by commas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD2CoXuh5TRI",
        "outputId": "db0b17bd-7ece-484e-f44e-8d1ee5a82a88"
      },
      "source": [
        "# titles = ['CRIM_Model_0020.mei']\n",
        "\n",
        "titles = ['Riquet_Missa_Susanne_1.mei_msg.mei', \n",
        "'Riquet_Missa_Susanne_2.mei_msg.mei', \n",
        "'Riquet_Missa_Susanne_3.mei_msg.mei', \n",
        "'Riquet_Missa_Susanne_4.mei_msg.mei', \n",
        "'Riquet_Missa_Susanne_5.mei_msg.mei']\n",
        "\n",
        "\n",
        "batch_classify(titles)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requesting file from https://raw.githubusercontent.com/RichardFreedman/CRIM_additional_works/main/Riquet_Missa_Susanne_1.mei_msg.mei...\n",
            "Successfully imported.\n",
            "Finding close matches...\n",
            "95 melodic intervals had more than 3 exact or close matches.\n",
            "\n",
            "Requesting file from https://raw.githubusercontent.com/RichardFreedman/CRIM_additional_works/main/Riquet_Missa_Susanne_2.mei_msg.mei...\n",
            "Successfully imported.\n",
            "Finding close matches...\n",
            "240 melodic intervals had more than 3 exact or close matches.\n",
            "\n",
            "Requesting file from https://raw.githubusercontent.com/RichardFreedman/CRIM_additional_works/main/Riquet_Missa_Susanne_3.mei_msg.mei...\n",
            "Successfully imported.\n",
            "Finding close matches...\n",
            "422 melodic intervals had more than 3 exact or close matches.\n",
            "\n",
            "Requesting file from https://raw.githubusercontent.com/RichardFreedman/CRIM_additional_works/main/Riquet_Missa_Susanne_4.mei_msg.mei...\n",
            "Successfully imported.\n",
            "Finding close matches...\n",
            "150 melodic intervals had more than 3 exact or close matches.\n",
            "\n",
            "Requesting file from https://raw.githubusercontent.com/RichardFreedman/CRIM_additional_works/main/Riquet_Missa_Susanne_5.mei_msg.mei...\n",
            "Successfully imported.\n",
            "Finding close matches...\n",
            "69 melodic intervals had more than 3 exact or close matches.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2gWwtU0jdAL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "a2deb09a-0e82-427c-d93e-c059185e0fc0"
      },
      "source": [
        "pivot_sort()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2ca1c2be8fd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpivot_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pivot_sort' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysCv5xyXjePi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}